{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC SBN Replica on BigQuery\n",
    "\n",
    "We've built a library called `mpcq` that was designed to interact at first with subscription the Small Bodies Node Postgres MPC databases. We've since ported those databases to Google BigQuery for faster queries.\n",
    "\n",
    "Contents\n",
    "- [Installation](#installation)\n",
    "- [Querying for Orbits](#querying-for-orbits)\n",
    "- [Querying for Observations](#querying-for-observations)\n",
    "- [Crossmatching against MPC Observations](#crossmatching-against-mpc-observations)\n",
    "- [Submission Information & Histories](#submission-information-&-histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "`mpcq` can be installed using pip:\n",
    "\n",
    "```bash\n",
    "pip install mpcq\n",
    "```\n",
    "\n",
    "Since we are working in the adam_core repository, let's add mpcq to the venv using pdm:\n",
    "\n",
    "```bash\n",
    "pdm add mpcq\n",
    "```\n",
    "\n",
    "#### Authenticating with Google \n",
    "\n",
    "You'll need to authenticate with Google to use the BigQuery client. See the mpcq documentation for more details.\n",
    "\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpcq.client import BigQueryMPCClient\n",
    "# add installation\n",
    "\n",
    "# Initialize client with your subscribed dataset IDs\n",
    "client = BigQueryMPCClient(\n",
    "   dataset_id=\"moeyens-thor-dev.mpc_sbn_aurora\",\n",
    "   views_dataset_id=\"moeyens-thor-dev.mpc_sbn_aurora_views\",\n",
    "   project=\"moeyens-thor-dev\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying for Orbits\n",
    "\n",
    "Let's start by querying for orbits. We'll use the `query_orbits` method to get the orbits for a list of provisional designations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch orbits for one or more objects\n",
    "orbits = client.query_orbits([\"2013 RR165\", \"2024 YR4\", \"A899 OF\"])\n",
    "\n",
    "# Basic analysis\n",
    "print(f\"Number of orbits: {len(orbits)}\")\n",
    "\n",
    "# You can view the data and get it as a pandas DataFrame\n",
    "orbits.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying for Observations\n",
    "\n",
    "We can also grab observations for a given list of provisional designations. One detail about the database structure is that observation of any one object can fall under any of its different provisional designations. We've taken a lot of care to ensure that you get all of the observations for an object, no matter which provisional designation it falls under and which one you query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = client.query_observations([\"A899 OF\"])\n",
    "observations.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossmatching against MPC Observations\n",
    "\n",
    "In some cases, especially working with archival data or in instances where you've decided to re-run source extraction pipelines, you may want to crossmatch your observations against the MPC observations. This is straightforward to do with the `cross_match_observations` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adam_core.observations import ADESObservations\n",
    "from adam_core.time import Timestamp\n",
    "\n",
    "input_observations = ADESObservations.from_kwargs(\n",
    "    # These are the only required columns for the cross-match\n",
    "    obsSubID=[\"1234567890\", \"1234567891\"],\n",
    "    obsTime=Timestamp.from_iso8601(['2011-01-30T11:15:25.920', '2011-01-30T11:37:22.656'], scale=\"utc\"),\n",
    "    ra=[123.884679, 123.880767],\n",
    "    dec=[19.820047, 19.820603],\n",
    "    stn=[\"F51\", \"F51\"],\n",
    "    astCat=[\"Gaia2\", \"Gaia2\"],\n",
    "    mode=[\"CCD\", \"CCD\"],\n",
    ")\n",
    "\n",
    "# Now you can cross-match the observations\n",
    "matched = client.cross_match_observations(input_observations)\n",
    "matched.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information & Histories\n",
    "\n",
    "We've also added a method to query the submission information and histories for a given provisional designations. These components are still under development as we build out our submission management system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get submission status\n",
    "observation_status = client.query_submission_info([\"2025-04-02T14:52:40.526_0000Gm0Q\"])\n",
    "observation_status.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get submission history\n",
    "history = client.query_submission_history([\"2024 YR4\"])\n",
    "history.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
